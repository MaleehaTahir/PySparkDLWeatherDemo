{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Direct Line Weather : ETL Workflow Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from py4j.java_gateway import java_import\n",
    "from datetime import datetime, time\n",
    "from py4j.protocol import Py4JJavaError\n",
    "from pyspark.sql import SparkSession, SQLContext, DataFrame\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import date_format\n",
    "from functools import reduce\n",
    "import pandas as pd\n",
    "import os\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_dir = 'C:\\DLTestFiles\\\\'\n",
    "zipped_dir = 'C:\\\\Users\\malee\\\\Downloads\\\\Data Engineer Test.zip'\n",
    "landing_path = [source_dir]\n",
    "clean_dir = 'clean\\weather\\\\'\n",
    "raw_folder = 'raw\\weather\\\\'\n",
    "base_dir = 'C:\\SparkDemo\\\\'\n",
    "temp_dir = 'tempdir\\\\'\n",
    "tempFiles, extracted_date = [], []\n",
    "df_dict = {}\n",
    "\n",
    "# ok lets create a spark session\n",
    "# you can set additional configs, however this would be dependent on the local resources you have available\n",
    "def createSparkSession(app_name, master='local'):\n",
    "    try:\n",
    "        session = SparkSession.builder.master(master).appName(app_name)\\\n",
    "                  .getOrCreate()\n",
    "    except Py4JJavaError as e:\n",
    "        raise e\n",
    "    return session\n",
    "\n",
    "\n",
    "sc = createSparkSession('HelloWorld') \n",
    "\n",
    "# this function allows us to visualise the file tree root structure \n",
    "def print_path_files(printpath):\n",
    "    import os\n",
    "    for root, dirs, files in os.walk(printpath):\n",
    "        level = root.replace(printpath, '').count(os.sep)\n",
    "        indent = ' ' * 4 * (level)\n",
    "        print('{}{}/'.format(indent, os.path.basename(root)))\n",
    "        subindent = ' ' * 4 * (level + 1)\n",
    "        for f in files:\n",
    "            print('{}{}'.format(subindent, f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "files have been successfully extracted!\n",
      "/\n",
      "    weather.20160201.csv\n",
      "    weather.20160301.csv\n"
     ]
    }
   ],
   "source": [
    "# extracting data from the zipped folder into the ingestion directory\n",
    "with zipfile.ZipFile(zipped_dir, \"r\") as zipObj:\n",
    "    try:\n",
    "        FileList = zipObj.namelist()\n",
    "        for file in FileList:\n",
    "            if file.startswith('weather'):\n",
    "                zipObj.extract(file, source_dir)\n",
    "        print(\"files have been successfully extracted!\")\n",
    "    except OSError as o:\n",
    "        print(str(o))\n",
    "        raise o\n",
    "        \n",
    "print_path_files(source_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/\n",
      "2016/\n",
      "    02/\n",
      "        01/\n",
      "            weather20160201.parquet\n",
      "    03/\n",
      "        01/\n",
      "            weather20160301.parquet\n"
     ]
    }
   ],
   "source": [
    "# setting hadoop properties with key value pairs\n",
    "# The aim of setting these properties is to build a bridge between pyspark and hdfs via java\n",
    "java_import(sc._jvm, 'org.apache.hadoop.fs.Path')\n",
    "sc._jsc.hadoopConfiguration().set(\"mapreduce.fileoutputcommitter.marksuccessfuljobs\", \"false\")\n",
    "sc._jsc.hadoopConfiguration().set(\"parquet.enable.summary-metadata.level\", \"false\")\n",
    "sc._jsc.hadoopConfiguration().set(\"spark.sql.sources.partitionColumnTypeInference.enabled\", \"true\")\n",
    "sc._jsc.hadoopConfiguration().set(\"spark.sql.execution.arrow.enabled\",\"true\")\n",
    "\n",
    "# ok, lets read the csv files from source and put them in the ingestion folder\n",
    "def split_file_date(sourcedir):\n",
    "    \"\"\"\n",
    "    This function is specifically used to extract the date from the\n",
    "    filenames and converting them to date format. The extracted\n",
    "    dates are then appended to a list, which is then returned from the\n",
    "    function.\n",
    "    \"\"\"\n",
    "    import glob\n",
    "    file_list = []\n",
    "    try:\n",
    "        for root, dirs, files in os.walk(sourcedir):\n",
    "            file_list += glob.glob(os.path.join(root, '*.csv'))\n",
    "            for i in file_list:\n",
    "                get_date = i.split(\".\")[1]\n",
    "                convert_to_date = datetime.strptime(get_date, '%Y%m%d').strftime('%Y/%m/%d').replace('/', '\\\\')\n",
    "                extracted_date.append(convert_to_date)\n",
    "    except OSError as e:\n",
    "        print(str(e))\n",
    "        raise e\n",
    "\n",
    "    return file_list\n",
    "\n",
    "\n",
    "# the for loop uses python's built-in enumerate function, with the udf defined above to create a dicitonary,\n",
    "# which stores the name of the dataframe associated with file that's being loaded into it.\n",
    "for f, df in enumerate(split_file_date(source_dir)):\n",
    "    df_dict['df_' + str(df).split(\".\")[1]] = sc.read.csv(df, header=True)\n",
    "\n",
    "# another for-loop is then used to iterate over the dict, from which the dataframe is extracted with it's associated name.\n",
    "# Remember, the name of the dataframe has been extracted from the date port of the filename in form of date so df_20160201\n",
    "for i, j in df_dict.items():\n",
    "    try:\n",
    "        date_split = datetime.strptime(i.split(\"_\")[1], '%Y%m%d').strftime('%Y/%m/%d').replace('/', '\\\\')\n",
    "        base_path = base_dir + raw_folder\n",
    "        full_path = base_path + date_split + '\\\\' + temp_dir\n",
    "        # ok, so spark by its nature as an MPP engine will partition files (partitions are dependent on the number of\n",
    "        # of executors defined- each executor is responsible for a task), which is usefui for a large workload\n",
    "        # however since, I am only working with a small dataset, I will merge all partitions into a single output.\n",
    "        j.repartition(1).write.mode(\"overwrite\").option(\"header\", \"true\").parquet(full_path) \n",
    "\n",
    "    except SystemError as e:\n",
    "        print(str(e))\n",
    "        raise e\n",
    "\n",
    "# list comprehension to save the raw_dir file paths to a list\n",
    "file_process_path = [base_dir+raw_folder + x for x in extracted_date]\n",
    "\n",
    "# please note that the function below is very similar to the one used to manage files in databricks, with Azure data lake\n",
    "def delete_from_temp(fullpath):\n",
    "    \"\"\"\n",
    "    param input: raw file path\n",
    "    When you originally write the parquet files to the fs, the naming convention\n",
    "    of the file remains undefined. Subsequently, you usually have a filename\n",
    "    that references the partition output id from the spark conversion.\n",
    "    The function below is used to leverage hdfs (spark pretty much sits on top of hdfs)\n",
    "    methods to create a temp dir, renaming the file and then deleting the temp folder.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        for l in fullpath:\n",
    "            fs = sc._jvm.org.apache.hadoop.fs.FileSystem.get(sc._jsc.hadoopConfiguration())\n",
    "            file = fs.globStatus(sc._jvm.Path(l + '\\\\' + temp_dir + 'part*'))[0].getPath().getName()\n",
    "            fs.rename(sc._jvm.Path(l + '\\\\' + temp_dir + file),\n",
    "                      sc._jvm.Path(l + '\\\\' + l[-19:].replace('\\\\', '') + '.parquet'))\n",
    "            fs.delete(sc._jvm.Path(l + '\\\\' + temp_dir), True)\n",
    "            fs.delete(sc._jvm.Path(l + '\\\\' + '.' + l[-19:].replace('\\\\', '') + '.parquet.crc'), True)\n",
    "    except IOError as e:\n",
    "        print(\"failed to delete the temp folder\" + str(e))\n",
    "        raise e\n",
    "\n",
    "    return fullpath\n",
    "\n",
    "\n",
    "ingestion_to_raw = delete_from_temp(file_process_path)\n",
    "\n",
    "print_path_files(base_dir+raw_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Quality Checks & Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sqlCtx = SQLContext(sc)\n",
    "# ok, so I have done a explicitly define both a spark and sql context, primarily for spark df to pandas df conversion.\n",
    "sqlCtx = SQLContext(sparkContext=sc.sparkContext, sparkSession=sc)\n",
    "\n",
    "clean_df = sqlCtx.read.option(\"inferSchema\", \"true\").parquet(*file_process_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The source file is not empty\n"
     ]
    }
   ],
   "source": [
    "# check to ensure that source file is not empty\n",
    "try:\n",
    "    if len(clean_df.head(1)) != 0:\n",
    "        print(\"The source file is not empty\")\n",
    "except IOError as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+--------------------+\n",
      "|      Type|RowCount|            filename|\n",
      "+----------+--------+--------------------+\n",
      "|SourceFile|   93255|weather.20160201.csv|\n",
      "|SourceFile|  101442|weather.20160301.csv|\n",
      "|TargetFile|   93255|weather20160201.p...|\n",
      "|TargetFile|  101442|weather20160301.p...|\n",
      "+----------+--------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# row_count comparison between source and target files, to ensure that we haven't lost any data between the file conversion\n",
    "# from csv to parquet\n",
    "def get_row_count(path, file_format, file_type, input):\n",
    "    \"\"\"\n",
    "    param input path: path to read file\n",
    "    param input file_format: file extension i.e. '.csv' '.parquet'\n",
    "    param input file_type: pyspark args; type of file\n",
    "    param input [input]: boolean True/False header property\n",
    "    row count is saved in a list of dicts, which is then\n",
    "    used to create a pandas & spark df. The function returns the\n",
    "    spark df\n",
    "    \"\"\"\n",
    "    new_row = []\n",
    "    try:\n",
    "        for i in path:\n",
    "            for root, dirs, files in os.walk(i):\n",
    "                for file in files:\n",
    "                    if file.endswith(file_format):\n",
    "                        f = os.path.join(root, file)\n",
    "                        row_count = sc.read.option(\"header\", input).format(file_type).load(f).count()\n",
    "                        new_row.append({'filename': file, 'RowCount': row_count})\n",
    "        row_df = pd.DataFrame(new_row)\n",
    "        sp_df = sqlCtx.createDataFrame(row_df)\n",
    "    except TypeError as t:\n",
    "        print(str(t))\n",
    "        raise AssertionError(t)\n",
    "\n",
    "    return sp_df\n",
    "\n",
    "\n",
    "def create_row_output(original_file, modified_file, source_file = [], target_file = []):\n",
    "    \"\"\"\n",
    "    param input original_file: df for original file\n",
    "    param input modified_file: df for modified file\n",
    "    param input source_file: list of paths for source files\n",
    "    param input target_file: list of paths for target files\n",
    "    The funciton from above is used to create the multiple source/target\n",
    "    df's, which are then merged for comparison purposes.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df_original = get_row_count(*source_file).withColumn(\"Type\", F.format_string(original_file))\n",
    "        df_modified = get_row_count(*target_file).withColumn(\"Type\", F.format_string(modified_file))\n",
    "        combined = df_original.union(df_modified)\n",
    "        cols = list(combined.columns)\n",
    "        cols = [cols[-1]] + cols[:-1]\n",
    "        rowComparisonDf = combined[cols]\n",
    "    except LookupError as l:\n",
    "        print(str(l))\n",
    "        raise l\n",
    "\n",
    "    return rowComparisonDf\n",
    "\n",
    "# the functions above can be used to specify various formats for comparison purposes\n",
    "array = []\n",
    "csv_file_rows = ['.csv', 'csv', 'true']  # use as source/target\n",
    "parquet_file_rows = ['.parquet', 'parquet', 'false']  # use as source/target\n",
    "\n",
    "create_row_output('SourceFile', 'TargetFile', [landing_path, *csv_file_rows], [file_process_path, *parquet_file_rows]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ForecastSiteCode: long (nullable = true)\n",
      " |-- ObservationTime: string (nullable = true)\n",
      " |-- ObservationDate: date (nullable = true)\n",
      " |-- WindDirection: integer (nullable = true)\n",
      " |-- WindSpeed: integer (nullable = true)\n",
      " |-- WindGust: integer (nullable = true)\n",
      " |-- Visibility: long (nullable = true)\n",
      " |-- ScreenTemperature: double (nullable = true)\n",
      " |-- Pressure: integer (nullable = true)\n",
      " |-- SignificantWeatherCode: integer (nullable = true)\n",
      " |-- SiteName: string (nullable = true)\n",
      " |-- Latitude: double (nullable = true)\n",
      " |-- Longitude: double (nullable = true)\n",
      " |-- Region: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Data Type Check\n",
    "# ok, typically when you infer schema from a pyspark dataframe, spark automatically gives you the best-fit data types \n",
    "# for the cols.\n",
    "# However, if you want to custom specify a schema, you can use the approach below. A benefit for the below approach is that\n",
    "# you might have, through your pre-processing data validation stage identified data types most suited to your dataset.\n",
    "# Additionally, you can pass in the lists below from various sources including as parameters to your pipeline. \n",
    "\n",
    "to_int_cols = ['WindSpeed', 'WindDirection', 'WindGust', 'Pressure', 'SignificantWeatherCode']\n",
    "to_long_cols = ['ForecastSiteCode', 'Visibility']\n",
    "to_date_cols = ['ObservationDate']\n",
    "to_double_cols = ['ScreenTemperature', 'Latitude', 'Longitude']\n",
    "\n",
    "# the assumption was made that the time fields in the weather datasets were of int type, and required formatting to \n",
    "# a time format\n",
    "clean_df = clean_df.withColumn('ObservationTime', F.lpad(clean_df['ObservationTime'], 4, '0').substr(3, 4))\n",
    "clean_df = clean_df.withColumn('ObservationTime', F.rpad(clean_df['ObservationTime'], 6, '0')).\\\n",
    "                    withColumn(\"ObservationTime\", (F.regexp_replace('ObservationTime',\"\"\"(\\d\\d)\"\"\", \"$1:\")).substr(0,8))\n",
    "\n",
    "# using a cast function from spark to modify the data types\n",
    "for col in clean_df.columns:\n",
    "    try:\n",
    "        if col in to_int_cols:\n",
    "            clean_df = clean_df.withColumn(col, F.col(col).cast('int'))\n",
    "        elif col in to_long_cols:\n",
    "            clean_df = clean_df.withColumn(col, F.col(col).cast('long'))\n",
    "        elif col in to_date_cols:\n",
    "            clean_df = clean_df.withColumn(col, F.col(col).cast('date'))\n",
    "        elif col in to_double_cols:\n",
    "            clean_df = clean_df.withColumn(col, F.col(col).cast('double'))\n",
    "        else:\n",
    "            pass\n",
    "    except AttributeError as ae:\n",
    "        print(str(ae))\n",
    "        raise ae\n",
    "\n",
    "# the df schema should now reflect the changes in the df data types\n",
    "clean_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null values have been handled\n",
      "-RECORD 0--------------------------------------------\n",
      " ForecastSiteCode       | 3002                       \n",
      " ObservationTime        | 00:00:00                   \n",
      " ObservationDate        | 2016-03-01                 \n",
      " WindDirection          | 8                          \n",
      " WindSpeed              | 23                         \n",
      " WindGust               | 30                         \n",
      " Visibility             | 16000                      \n",
      " ScreenTemperature      | -99.0                      \n",
      " Pressure               | 0                          \n",
      " SignificantWeatherCode | 8                          \n",
      " SiteName               | BALTASOUND (3002)          \n",
      " Latitude               | 60.749                     \n",
      " Longitude              | -0.854                     \n",
      " Region                 | Orkney & Shetland          \n",
      " Country                | SCOTLAND                   \n",
      "-RECORD 1--------------------------------------------\n",
      " ForecastSiteCode       | 3005                       \n",
      " ObservationTime        | 00:00:00                   \n",
      " ObservationDate        | 2016-03-01                 \n",
      " WindDirection          | 8                          \n",
      " WindSpeed              | 26                         \n",
      " WindGust               | 34                         \n",
      " Visibility             | 5000                       \n",
      " ScreenTemperature      | 4.9                        \n",
      " Pressure               | 1004                       \n",
      " SignificantWeatherCode | 12                         \n",
      " SiteName               | LERWICK (S. SCREEN) (3005) \n",
      " Latitude               | 60.139                     \n",
      " Longitude              | -1.183                     \n",
      " Region                 | Orkney & Shetland          \n",
      " Country                | SCOTLAND                   \n",
      "-RECORD 2--------------------------------------------\n",
      " ForecastSiteCode       | 3008                       \n",
      " ObservationTime        | 00:00:00                   \n",
      " ObservationDate        | 2016-03-01                 \n",
      " WindDirection          | 7                          \n",
      " WindSpeed              | 30                         \n",
      " WindGust               | 40                         \n",
      " Visibility             | 5000                       \n",
      " ScreenTemperature      | 5.1                        \n",
      " Pressure               | 1003                       \n",
      " SignificantWeatherCode | 11                         \n",
      " SiteName               | FAIR ISLE (3008)           \n",
      " Latitude               | 59.53                      \n",
      " Longitude              | -1.63                      \n",
      " Region                 | Orkney & Shetland          \n",
      " Country                | Unknown                    \n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# handling NULLS\n",
    "# It's good practice to handle Nulls and blanks in your dataset.\n",
    "# Below, using list comprehension all categorical and continous fields have been added to lists\n",
    "all_str_cols = [item[0] for item in clean_df.dtypes if item[1].startswith('string')]\n",
    "all_continuous_cols =[item[0] for item in clean_df.dtypes if item[1].startswith(('int', 'long', 'double'))]\n",
    "try:\n",
    "    clean_df = clean_df.na.fill(\"Unknown\", all_str_cols)\n",
    "    clean_df = clean_df.na.fill(0, all_continuous_cols)\n",
    "    default_time = '00:00:00'\n",
    "    clean_df = clean_df.fillna({'ObservationTime': default_time})\n",
    "    print(\"Null values have been handled\")\n",
    "except Exception as e:\n",
    "    print(str(e))\n",
    "    raise e\n",
    "\n",
    "clean_df.show(n=3, truncate=False, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/\n",
      "2016/\n",
      "    02/\n",
      "        01/\n",
      "            weather20160201.parquet\n",
      "        02/\n",
      "            weather20160202.parquet\n",
      "        03/\n",
      "            weather20160203.parquet\n",
      "        04/\n",
      "            weather20160204.parquet\n",
      "        05/\n",
      "            weather20160205.parquet\n",
      "        06/\n",
      "            weather20160206.parquet\n",
      "        07/\n",
      "            weather20160207.parquet\n",
      "        08/\n",
      "            weather20160208.parquet\n",
      "        09/\n",
      "            weather20160209.parquet\n",
      "        10/\n",
      "            weather20160210.parquet\n",
      "        11/\n",
      "            weather20160211.parquet\n",
      "        12/\n",
      "            weather20160212.parquet\n",
      "        13/\n",
      "            weather20160213.parquet\n",
      "        14/\n",
      "            weather20160214.parquet\n",
      "        15/\n",
      "            weather20160215.parquet\n",
      "        16/\n",
      "            weather20160216.parquet\n",
      "        17/\n",
      "            weather20160217.parquet\n",
      "        18/\n",
      "            weather20160218.parquet\n",
      "        19/\n",
      "            weather20160219.parquet\n",
      "        20/\n",
      "            weather20160220.parquet\n",
      "        21/\n",
      "            weather20160221.parquet\n",
      "        22/\n",
      "            weather20160222.parquet\n",
      "        23/\n",
      "            weather20160223.parquet\n",
      "        24/\n",
      "            weather20160224.parquet\n",
      "        25/\n",
      "            weather20160225.parquet\n",
      "        26/\n",
      "            weather20160226.parquet\n",
      "        27/\n",
      "            weather20160227.parquet\n",
      "        28/\n",
      "            weather20160228.parquet\n",
      "        29/\n",
      "            weather20160229.parquet\n",
      "    03/\n",
      "        01/\n",
      "            weather20160301.parquet\n",
      "        02/\n",
      "            weather20160302.parquet\n",
      "        03/\n",
      "            weather20160303.parquet\n",
      "        04/\n",
      "            weather20160304.parquet\n",
      "        05/\n",
      "            weather20160305.parquet\n",
      "        06/\n",
      "            weather20160306.parquet\n",
      "        07/\n",
      "            weather20160307.parquet\n",
      "        08/\n",
      "            weather20160308.parquet\n",
      "        09/\n",
      "            weather20160309.parquet\n",
      "        10/\n",
      "            weather20160310.parquet\n",
      "        11/\n",
      "            weather20160311.parquet\n",
      "        12/\n",
      "            weather20160312.parquet\n",
      "        13/\n",
      "            weather20160313.parquet\n",
      "        14/\n",
      "            weather20160314.parquet\n",
      "        15/\n",
      "            weather20160315.parquet\n",
      "        16/\n",
      "            weather20160316.parquet\n",
      "        17/\n",
      "            weather20160317.parquet\n",
      "        18/\n",
      "            weather20160318.parquet\n",
      "        19/\n",
      "            weather20160319.parquet\n",
      "        20/\n",
      "            weather20160320.parquet\n",
      "        21/\n",
      "            weather20160321.parquet\n",
      "        22/\n",
      "            weather20160322.parquet\n",
      "        23/\n",
      "            weather20160323.parquet\n",
      "        24/\n",
      "            weather20160324.parquet\n",
      "        25/\n",
      "            weather20160325.parquet\n",
      "        26/\n",
      "            weather20160326.parquet\n",
      "        27/\n",
      "            weather20160327.parquet\n",
      "        28/\n",
      "            weather20160328.parquet\n",
      "        29/\n",
      "            weather20160329.parquet\n",
      "        30/\n",
      "            weather20160330.parquet\n",
      "        31/\n",
      "            weather20160331.parquet\n"
     ]
    }
   ],
   "source": [
    "# ok, I am going to revert to pandas for a bit\n",
    "# The observation date is converted into datetime, so we can set an index on the pandas dataframe,\n",
    "# the date column is then formatted to a date from datetime\n",
    "pd_df = clean_df.toPandas()\n",
    "pd_df['ObservationDate'] = pd.to_datetime(pd_df['ObservationDate'], format='%Y-%m-%d')\n",
    "pd_df['ObservedDate'] = pd_df['ObservationDate'].dt.date\n",
    "\n",
    "# we could've also used the pyspark approach defined above.\n",
    "def create_file_paritions(file_prefix='weather'):\n",
    "    \"\"\"\n",
    "    This function is used to create daily file partitions\n",
    "    from date taken from the filename. This is a pandas approach,\n",
    "    where we group data from the date column in the dataframe. The\n",
    "    for-loop is then to used to write the files to target dir based\n",
    "    on the distinct yyyy/mm/dd specified in the date column. This \n",
    "    could also be adjusted for a higher level categorization i.e. YYYY/mm\n",
    "    \"\"\"\n",
    "    partitioned_files = pd_df.groupby([pd_df.ObservationDate.dt.date])\n",
    "    processed_df_list = []\n",
    "    try:\n",
    "        for (ObservationDate), group in partitioned_files:\n",
    "            fname = file_prefix + ObservationDate.strftime('%Y%m%d')\n",
    "            clean_process_path = base_dir + clean_dir + str(ObservationDate).replace('-', '\\\\')\n",
    "            group.iloc[:, 0:].to_parquet(clean_process_path + '\\\\' + f'{fname}.parquet')\n",
    "            # the files are then read back into a pandas df and df's ar appended to a list\n",
    "            processed_df_list.append(pd.read_parquet(clean_process_path))\n",
    "    except RuntimeError as re:\n",
    "        print(str(re))\n",
    "        raise re\n",
    "\n",
    "    return processed_df_list\n",
    "\n",
    "print_path_files(base_dir+clean_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Temperature on date of occurrence (2016-03-17): 15.8\n",
      "Maximum Temperature was recorded in the region Highland & Eilean Siar: 15.8\n"
     ]
    }
   ],
   "source": [
    "# the pandas concat function is used to merge dfs in the list into a single dataframe\n",
    "processed_df = pd.concat(create_file_paritions(), ignore_index=True)\n",
    "processed_df.drop('ObservationDate', axis=1, inplace=True)\n",
    "processed_df.rename(columns={'ObservedDate': 'ObservationDate'}, inplace=True)\n",
    "\n",
    "\n",
    "# grouped data by date\n",
    "group_temp_by_day = processed_df.groupby(['ObservationDate'], sort=False)['ScreenTemperature'].max()\n",
    "\n",
    "\n",
    "# DL Queries\n",
    "# first get the max screentemp\n",
    "maxTemp = processed_df['ScreenTemperature'].max()\n",
    "# second map the maxtemp var to date column in df to extract the associated date\n",
    "date_of_max_temp = processed_df[processed_df['ScreenTemperature'] == maxTemp]['ObservationDate'].values[0]\n",
    "# third, map the maxtemp var to region column to extract the associated date\n",
    "region_of_max_temp = processed_df[processed_df['ScreenTemperature'] == maxTemp]['Region'].values[0]\n",
    "# combining the above three vars into a single statement\n",
    "filtered_max_temp = processed_df[(processed_df['ObservationDate'] == date_of_max_temp) &\n",
    "                                 (processed_df['Region'] == region_of_max_temp) &\n",
    "                                 (processed_df['ScreenTemperature'] == maxTemp)]\n",
    "# saving output to a list\n",
    "output = filtered_max_temp[['ObservationDate', 'Region', 'ScreenTemperature']]\n",
    "print('Maximum Temperature on date of occurrence' + \" \" + \"(\" + str(date_of_max_temp) + \")\" + ':' + \" \" + str(maxTemp))\n",
    "print('Maximum Temperature was recorded in the region' + \" \" + str(region_of_max_temp) + ':' + \" \" + str(maxTemp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+\n",
      "|max(ScreenTemperature)|\n",
      "+----------------------+\n",
      "|                  15.8|\n",
      "+----------------------+\n",
      "\n",
      "+--------------------+\n",
      "|              Region|\n",
      "+--------------------+\n",
      "|Highland & Eilean...|\n",
      "+--------------------+\n",
      "\n",
      "+---------------+--------------------+-----------------+\n",
      "|ObservationDate|              Region|ScreenTemperature|\n",
      "+---------------+--------------------+-----------------+\n",
      "|     2016-03-17|Highland & Eilean...|             15.8|\n",
      "+---------------+--------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# revert pandas dataframe to spark dataframe\n",
    "processed_spark = sqlCtx.createDataFrame(processed_df)\n",
    "# processed_spark.printSchema()\n",
    "\n",
    "\"\"\"\n",
    "The spark sql context can be used to register a temp table (in-memory) to make sql queries.\n",
    "The output of both the python and sql queries should be identical (and well it is!!)\n",
    "\"\"\"\n",
    "try:\n",
    "    processed_spark.registerTempTable(\"Weather\")\n",
    "    sql_max_temp = sqlCtx.sql(\"select MAX(ScreenTemperature) from Weather\").show()\n",
    "    sql_max_dtemp_date = sqlCtx.sql(\"SELECT ObservationDate FROM Weather WHERE ScreenTemperature = \"\n",
    "                                    \"(SELECT MAX(ScreenTemperature) FROM Weather)\")\n",
    "    sql_max_region = sqlCtx.sql(\"SELECT Region FROM Weather WHERE ScreenTemperature = \"\n",
    "                                \"(SELECT MAX(ScreenTemperature) FROM Weather)\").show()\n",
    "    sql_max_aggreagted = sqlCtx.sql(\"SELECT ObservationDate, Region, ScreenTemperature \"\n",
    "                                    \"FROM Weather WHERE ScreenTemperature = \"\n",
    "                                    \"(SELECT MAX(ScreenTemperature) FROM Weather)\").show()\n",
    "    sqlCtx.sql(\"DROP TABLE Weather\")\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    raise e"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
